{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from glob import glob \n",
    "import time\n",
    "import random\n",
    "from datetime import datetime\n",
    "\n",
    "import timm\n",
    "import torch\n",
    "import albumentations as A\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch.nn as nn\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "from torch.optim import Adam\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "import cv2\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_seed(seed):\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "\n",
    "random_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# data config\n",
    "train_data_path = '../data/train/'\n",
    "test_data_path = '../data/test/'\n",
    "\n",
    "# model config\n",
    "model_name = 'efficientnet_b4' #resnet50' 'efficientnet-b0', ...\n",
    "\n",
    "# training config\n",
    "img_size = 384\n",
    "LR = 1e-4\n",
    "EPOCHS = 20\n",
    "BATCH_SIZE = 16\n",
    "num_workers = 0\n",
    "log_interval = 1413"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터셋 클래스를 정의합니다.\n",
    "class ImageDataset(Dataset):\n",
    "    def __init__(self, data_path, ids, targets=None, transform=None):\n",
    "        self.ids = ids\n",
    "        self.targets = targets\n",
    "        self.data_path = data_path\n",
    "        self.transform = transform \n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.ids)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        ID = self.ids[idx]\n",
    "        image_path = os.path.join(self.data_path, ID)\n",
    "        img = np.array(Image.open(image_path))\n",
    "        if self.transform:\n",
    "            img = self.transform(image=img)['image']\n",
    "        if self.targets is not None:\n",
    "            return img, self.targets[idx]\n",
    "        else:\n",
    "            return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mixup_data(x, y, alpha=1.0):\n",
    "    if alpha > 0:\n",
    "        lam = np.random.beta(alpha, alpha)\n",
    "    else:\n",
    "        lam = 1\n",
    "\n",
    "    batch_size = x.size()[0]\n",
    "    index = torch.randperm(batch_size)\n",
    "    mixed_x = lam * x + (1 - lam) * x[index, :]\n",
    "    y_a, y_b = y, y[index]\n",
    "    return mixed_x, y_a, y_b, lam\n",
    "\n",
    "def mixup_loss(loss_fn, pred, labels_a, labels_b, lam):\n",
    "    return lam * loss_fn(pred, labels_a) + (1 - lam) * loss_fn(pred, labels_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_per_epoch(epoch, model, train_loader, loss_fn, optimizer, train_step):\n",
    "    preds_list = []\n",
    "    labels_list = []\n",
    "\n",
    "    for batch_idx, (images, labels) in enumerate(train_loader):\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "        current_loss = 0.0\n",
    "\n",
    "        if (batch_idx + 1) % 5 == 0:\n",
    "            imgs, labels_a, labels_b, lambda_ = mixup_data(images, labels)\n",
    "            output = model(imgs)\n",
    "            loss = mixup_loss(loss_fn=loss_fn, pred=output, labels_a=labels_a, labels_b=labels_b, lam=lambda_)\n",
    "        else: \n",
    "            outputs = model(images)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            loss = loss_fn(outputs, labels)\n",
    "\n",
    "            preds_list.extend(preds.detach().cpu().numpy())\n",
    "            labels_list.extend(labels.detach().cpu().numpy())\n",
    "\n",
    "        # Back propagation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        current_loss += loss.item()\n",
    "\n",
    "        if (batch_idx + 1) % log_interval == 0:\n",
    "            train_loss = current_loss / log_interval\n",
    "            train_acc = accuracy_score(labels_list, preds_list)\n",
    "            train_f1 = f1_score(labels_list, preds_list, average='macro')\n",
    "\n",
    "            print(\"Train Epoch: {} [{}/{} ({:.0f}%)]\\tTrain Loss: {:.6f}, Train Acc: {:.6f}, Train F1 Score: {:.6f}\".format(\n",
    "                epoch, \n",
    "                batch_idx * len(images), \n",
    "                len(train_loader) * len(images), \n",
    "                100 * batch_idx / len(train_loader), \n",
    "                train_loss, \n",
    "                train_acc,\n",
    "                train_f1))\n",
    "\n",
    "            if wandb is not None:\n",
    "                wandb.log({\n",
    "                    \"Loss/Train\": train_loss,\n",
    "                    \"Accuracy/Train\": train_acc,\n",
    "                    \"F1 Score/Train\": train_f1,\n",
    "                    \"Image/Train\": wandb.Image(images),\n",
    "                    \"Output/Train\": wandb.Histogram(outputs.detach().cpu().numpy()),\n",
    "                    \"Preds/Train\": wandb.Histogram(preds.detach().cpu().numpy()),\n",
    "                    \"Labels/Train\": wandb.Histogram(labels.detach().cpu().numpy()),  \n",
    "                }, step=train_step)\n",
    "        train_step += 1\n",
    "\n",
    "    return train_step, current_loss / len(train_loader), accuracy_score(labels_list, preds_list), f1_score(labels_list, preds_list, average='macro')\n",
    "    \n",
    "def validation(model, val_loader, loss_fn, train_step):\n",
    "    val_preds_list = []\n",
    "    val_labels_list = []\n",
    "    with torch.no_grad():\n",
    "        val_loss = 0.0\n",
    "        model.eval()\n",
    "\n",
    "        for val_idx, (val_images, val_labels) in enumerate(val_loader):\n",
    "            val_images, val_labels = val_images.to(device), val_labels.to(device)\n",
    "\n",
    "            val_outputs = model(val_images)\n",
    "            _, val_preds = torch.max(val_outputs, 1)\n",
    "\n",
    "            val_loss += loss_fn(val_outputs, val_labels) / val_outputs.shape[0]\n",
    "            \n",
    "            val_preds_list.extend(val_preds.detach().cpu().numpy())\n",
    "            val_labels_list.extend(val_labels.detach().cpu().numpy())\n",
    "\n",
    "        val_epoch_loss = val_loss / len(val_loader)\n",
    "        val_epoch_acc = accuracy_score(val_labels_list, val_preds_list)\n",
    "        val_epoch_f1 = f1_score(val_labels_list, val_preds_list, average='macro')\n",
    "\n",
    "        if wandb is not None:\n",
    "            wandb.log({\n",
    "                \"Loss/Val\": val_epoch_loss,\n",
    "                \"Accuracy/Val\": val_epoch_acc,\n",
    "                \"F1 Score/Val\": val_epoch_f1,\n",
    "                \"Image/Val\": wandb.Image(val_images),\n",
    "                \"Output/Val\": wandb.Histogram(val_outputs.detach().cpu().numpy()),\n",
    "                \"Preds/Val\": wandb.Histogram(val_preds.detach().cpu().numpy()),\n",
    "                \"Labels/Val\": wandb.Histogram(val_labels.detach().cpu().numpy()),  \n",
    "            }, step=train_step )\n",
    "\n",
    "        print(\"Validation dataset: Val Loss: {:.6f}, Val Acc: {:.6f}, Val F1 Score: {:.6f}\".format(val_epoch_loss, val_epoch_acc, val_epoch_f1))\n",
    "    return val_epoch_loss, val_epoch_acc, val_epoch_f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../data/new_aug_train.csv')\n",
    "sample_submission_df = pd.read_csv(\"../data/sample_submission.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "113068 28268\n"
     ]
    }
   ],
   "source": [
    "df_train, df_val = train_test_split(df, test_size=0.2, shuffle=True, random_state=42)\n",
    "print(len(df_train), len(df_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_transform = A.Compose([\n",
    "    A.ShiftScaleRotate(shift_limit=0.2, scale_limit=0, rotate_limit=90, p=0.5, border_mode=0, value=(255, 255, 255)),\n",
    "    # 이미지 크기 조정\n",
    "    A.Resize(height=img_size, width=img_size),\n",
    "    # images normalization\n",
    "    A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    # numpy 이미지나 PIL 이미지를 PyTorch 텐서로 변환\n",
    "    ToTensorV2(),\n",
    "])\n",
    "\n",
    "# test image 변환을 위한 transform 코드\n",
    "valid_transform = A.Compose([\n",
    "    A.ShiftScaleRotate(shift_limit=0.2, scale_limit=0, rotate_limit=90, p=0.5, border_mode=0, value=(255, 255, 255)),\n",
    "    A.Resize(height=img_size, width=img_size),\n",
    "    A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    ToTensorV2(),\n",
    "])\n",
    "\n",
    "test_transform = A.Compose([\n",
    "    A.Resize(height=img_size, width=img_size),\n",
    "    A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    ToTensorV2(),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "113068 28268 3140\n"
     ]
    }
   ],
   "source": [
    "# Dataset 정의\n",
    "train_dataset = ImageDataset(\n",
    "    train_data_path,\n",
    "    df_train['ID'].values,\n",
    "    df_train['target'].values,\n",
    "    transform=train_transform\n",
    ")\n",
    "valid_dataset = ImageDataset(\n",
    "    train_data_path,\n",
    "    df_val['ID'].values,\n",
    "    df_val['target'].values,\n",
    "    transform=valid_transform\n",
    ")\n",
    "test_dataset = ImageDataset(\n",
    "    test_data_path,\n",
    "    sample_submission_df['ID'].values,\n",
    "    transform=test_transform\n",
    ")\n",
    "print(len(train_dataset), len(valid_dataset), len(test_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DataLoader 정의\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    num_workers=num_workers,\n",
    "    pin_memory=True,\n",
    "    drop_last=False\n",
    ")\n",
    "valid_loader = DataLoader(\n",
    "    valid_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False,\n",
    "    num_workers=0,\n",
    "    pin_memory=True\n",
    ")\n",
    "test_loader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False,\n",
    "    num_workers=0,\n",
    "    pin_memory=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_weights = torch.tensor([1.0, 1.0, 1.0, 3.0, 2.0, 1.0, 1.0, 3.0, 1.0, 1.0, 1.0, 1.0, 2.0, 1.0, 2.0, 1.0, 1.0]).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mch_hee\u001b[0m (\u001b[33maistages-cv-04\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.3"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/data/ephemeral/home/code/wandb/run-20240219_113335-gpnb03ov</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/aistages-cv-04/DTC-ch/runs/gpnb03ov' target=\"_blank\">2024-02-19T11:33:33-efficientnet_b4-Adam_optim-0.0001_lr_using_class-weights_new_aug_data_16_batch_</a></strong> to <a href='https://wandb.ai/aistages-cv-04/DTC-ch' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/aistages-cv-04/DTC-ch' target=\"_blank\">https://wandb.ai/aistages-cv-04/DTC-ch</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/aistages-cv-04/DTC-ch/runs/gpnb03ov' target=\"_blank\">https://wandb.ai/aistages-cv-04/DTC-ch/runs/gpnb03ov</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/aistages-cv-04/DTC-ch/runs/gpnb03ov?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x7fa0bc6182e0>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = timm.create_model(\n",
    "    model_name,\n",
    "    pretrained=True,\n",
    "    num_classes=17\n",
    ").to(device)\n",
    "loss_fn = nn.CrossEntropyLoss(weight=class_weights)\n",
    "optimizer = Adam(model.parameters(), lr=LR)\n",
    "optimizer_name = type(optimizer).__name__\n",
    "# scheduler = torch.optim.lr_scheduler.CyclicLR(optimizer, base_lr=5e-5, max_lr=1e-3, step_size_up=5, mode=\"triangular2\")\n",
    "run_name = f\"{datetime.now().isoformat(timespec='seconds')}-{model_name}-{optimizer_name}_optim-{LR}_lr_using_class-weights_new_aug_data_16_batch_\"\n",
    "log_dir = f\"runs/{run_name}\"\n",
    "\n",
    "project_name = 'DTC-ch'\n",
    "run_tags=[project_name]\n",
    "wandb.init(\n",
    "    project=project_name,\n",
    "    name=run_name,\n",
    "    tags=run_tags,\n",
    "    config={\"lr\": LR, \"model_name\": model_name, \"optimizer_name\": optimizer_name},\n",
    "    reinit=True\n",
    ")\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_dir = f\"runs/{datetime.now().isoformat(timespec='seconds')}-{model_name}_new_aug_data_16_batch_\"\n",
    "model_path = os.path.join(log_dir, 'models')\n",
    "os.makedirs(model_path, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [22592/113072 (20%)]\tTrain Loss: 0.000137, Train Acc: 0.751547, Train F1 Score: 0.758325\n",
      "Train Epoch: 1 [45200/113072 (40%)]\tTrain Loss: 0.000455, Train Acc: 0.837904, Train F1 Score: 0.841702\n",
      "Train Epoch: 1 [67808/113072 (60%)]\tTrain Loss: 0.000005, Train Acc: 0.877709, Train F1 Score: 0.880228\n",
      "Train Epoch: 1 [90416/113072 (80%)]\tTrain Loss: 0.000184, Train Acc: 0.901178, Train F1 Score: 0.902815\n",
      "Train Epoch: 1 [113024/113072 (100%)]\tTrain Loss: 0.001012, Train Acc: 0.916700, Train F1 Score: 0.917943\n",
      "Validation dataset: Val Loss: 0.004819, Val Acc: 0.984859, Val F1 Score: 0.985055\n",
      "Saved Model!!!\n",
      "Train Epoch: 2 [22592/113072 (20%)]\tTrain Loss: 0.000010, Train Acc: 0.980824, Train F1 Score: 0.980974\n",
      "Train Epoch: 2 [45200/113072 (40%)]\tTrain Loss: 0.000021, Train Acc: 0.984465, Train F1 Score: 0.984539\n",
      "Train Epoch: 2 [67808/113072 (60%)]\tTrain Loss: 0.000023, Train Acc: 0.986568, Train F1 Score: 0.986607\n",
      "Train Epoch: 2 [90416/113072 (80%)]\tTrain Loss: 0.000016, Train Acc: 0.987768, Train F1 Score: 0.987842\n",
      "Train Epoch: 2 [113024/113072 (100%)]\tTrain Loss: 0.000350, Train Acc: 0.988699, Train F1 Score: 0.988736\n",
      "Validation dataset: Val Loss: 0.003372, Val Acc: 0.988857, Val F1 Score: 0.989077\n",
      "Saved Model!!!\n",
      "Train Epoch: 3 [22592/113072 (20%)]\tTrain Loss: 0.000002, Train Acc: 0.994640, Train F1 Score: 0.994598\n",
      "Train Epoch: 3 [45200/113072 (40%)]\tTrain Loss: 0.000014, Train Acc: 0.994748, Train F1 Score: 0.994766\n",
      "Train Epoch: 3 [67808/113072 (60%)]\tTrain Loss: 0.000138, Train Acc: 0.994970, Train F1 Score: 0.994979\n",
      "Train Epoch: 3 [90416/113072 (80%)]\tTrain Loss: 0.000047, Train Acc: 0.995066, Train F1 Score: 0.995075\n",
      "Train Epoch: 3 [113024/113072 (100%)]\tTrain Loss: 0.000093, Train Acc: 0.995322, Train F1 Score: 0.995357\n",
      "Validation dataset: Val Loss: 0.002099, Val Acc: 0.993172, Val F1 Score: 0.993327\n",
      "Saved Model!!!\n",
      "Train Epoch: 4 [22592/113072 (20%)]\tTrain Loss: 0.000183, Train Acc: 0.996242, Train F1 Score: 0.996195\n",
      "Train Epoch: 4 [45200/113072 (40%)]\tTrain Loss: 0.000003, Train Acc: 0.996683, Train F1 Score: 0.996722\n",
      "Train Epoch: 4 [67808/113072 (60%)]\tTrain Loss: 0.000001, Train Acc: 0.996978, Train F1 Score: 0.997013\n",
      "Train Epoch: 4 [90416/113072 (80%)]\tTrain Loss: 0.000000, Train Acc: 0.997028, Train F1 Score: 0.997050\n",
      "Train Epoch: 4 [113024/113072 (100%)]\tTrain Loss: 0.000762, Train Acc: 0.997070, Train F1 Score: 0.997096\n",
      "Validation dataset: Val Loss: 0.001251, Val Acc: 0.995649, Val F1 Score: 0.995723\n",
      "Saved Model!!!\n",
      "Train Epoch: 5 [22592/113072 (20%)]\tTrain Loss: 0.000173, Train Acc: 0.997182, Train F1 Score: 0.997192\n",
      "Train Epoch: 5 [45200/113072 (40%)]\tTrain Loss: 0.000004, Train Acc: 0.997512, Train F1 Score: 0.997525\n",
      "Train Epoch: 5 [67808/113072 (60%)]\tTrain Loss: 0.000001, Train Acc: 0.997568, Train F1 Score: 0.997568\n",
      "Train Epoch: 5 [90416/113072 (80%)]\tTrain Loss: 0.000005, Train Acc: 0.997692, Train F1 Score: 0.997702\n",
      "Train Epoch: 5 [113024/113072 (100%)]\tTrain Loss: 0.000204, Train Acc: 0.997722, Train F1 Score: 0.997731\n",
      "Validation dataset: Val Loss: 0.000717, Val Acc: 0.997241, Val F1 Score: 0.997256\n",
      "Saved Model!!!\n",
      "Train Epoch: 6 [22592/113072 (20%)]\tTrain Loss: 0.000001, Train Acc: 0.997845, Train F1 Score: 0.997861\n",
      "Train Epoch: 6 [45200/113072 (40%)]\tTrain Loss: 0.000001, Train Acc: 0.997899, Train F1 Score: 0.997922\n",
      "Train Epoch: 6 [67808/113072 (60%)]\tTrain Loss: 0.000001, Train Acc: 0.998028, Train F1 Score: 0.998039\n",
      "Train Epoch: 6 [90416/113072 (80%)]\tTrain Loss: 0.000001, Train Acc: 0.998189, Train F1 Score: 0.998196\n",
      "Train Epoch: 6 [113024/113072 (100%)]\tTrain Loss: 0.000714, Train Acc: 0.998131, Train F1 Score: 0.998137\n",
      "Validation dataset: Val Loss: 0.000841, Val Acc: 0.997594, Val F1 Score: 0.997615\n",
      "Saved Model!!!\n",
      "Train Epoch: 7 [22592/113072 (20%)]\tTrain Loss: 0.000006, Train Acc: 0.998563, Train F1 Score: 0.998588\n",
      "Train Epoch: 7 [45200/113072 (40%)]\tTrain Loss: 0.000000, Train Acc: 0.998590, Train F1 Score: 0.998605\n",
      "Train Epoch: 7 [67808/113072 (60%)]\tTrain Loss: 0.000002, Train Acc: 0.998802, Train F1 Score: 0.998813\n",
      "Train Epoch: 7 [90416/113072 (80%)]\tTrain Loss: 0.000001, Train Acc: 0.998770, Train F1 Score: 0.998776\n",
      "Train Epoch: 7 [113024/113072 (100%)]\tTrain Loss: 0.001035, Train Acc: 0.998651, Train F1 Score: 0.998656\n",
      "Validation dataset: Val Loss: 0.000573, Val Acc: 0.998302, Val F1 Score: 0.998305\n",
      "Saved Model!!!\n",
      "Train Epoch: 8 [22592/113072 (20%)]\tTrain Loss: 0.000001, Train Acc: 0.999061, Train F1 Score: 0.999047\n",
      "Train Epoch: 8 [45200/113072 (40%)]\tTrain Loss: 0.000001, Train Acc: 0.998728, Train F1 Score: 0.998730\n",
      "Train Epoch: 8 [67808/113072 (60%)]\tTrain Loss: 0.000009, Train Acc: 0.998710, Train F1 Score: 0.998712\n",
      "Train Epoch: 8 [90416/113072 (80%)]\tTrain Loss: 0.000000, Train Acc: 0.998784, Train F1 Score: 0.998786\n",
      "Train Epoch: 8 [113024/113072 (100%)]\tTrain Loss: 0.000605, Train Acc: 0.998750, Train F1 Score: 0.998752\n",
      "Validation dataset: Val Loss: 0.000976, Val Acc: 0.997347, Val F1 Score: 0.997391\n",
      "Train Epoch: 9 [22592/113072 (20%)]\tTrain Loss: 0.000002, Train Acc: 0.999061, Train F1 Score: 0.999062\n",
      "Train Epoch: 9 [45200/113072 (40%)]\tTrain Loss: 0.000001, Train Acc: 0.999033, Train F1 Score: 0.999044\n",
      "Train Epoch: 9 [67808/113072 (60%)]\tTrain Loss: 0.000000, Train Acc: 0.999023, Train F1 Score: 0.999029\n",
      "Train Epoch: 9 [90416/113072 (80%)]\tTrain Loss: 0.000006, Train Acc: 0.999033, Train F1 Score: 0.999039\n",
      "Train Epoch: 9 [113024/113072 (100%)]\tTrain Loss: 0.000519, Train Acc: 0.998938, Train F1 Score: 0.998945\n",
      "Validation dataset: Val Loss: 0.000497, Val Acc: 0.998833, Val F1 Score: 0.998847\n",
      "Saved Model!!!\n",
      "Train Epoch: 10 [22592/113072 (20%)]\tTrain Loss: 0.000003, Train Acc: 0.999061, Train F1 Score: 0.999073\n",
      "Train Epoch: 10 [45200/113072 (40%)]\tTrain Loss: 0.000001, Train Acc: 0.998867, Train F1 Score: 0.998874\n",
      "Train Epoch: 10 [67808/113072 (60%)]\tTrain Loss: 0.000000, Train Acc: 0.999023, Train F1 Score: 0.999026\n",
      "Train Epoch: 10 [90416/113072 (80%)]\tTrain Loss: 0.000000, Train Acc: 0.999033, Train F1 Score: 0.999035\n",
      "Train Epoch: 10 [113024/113072 (100%)]\tTrain Loss: 0.000663, Train Acc: 0.999093, Train F1 Score: 0.999094\n",
      "Validation dataset: Val Loss: 0.000312, Val Acc: 0.998726, Val F1 Score: 0.998745\n",
      "Train Epoch: 11 [22592/113072 (20%)]\tTrain Loss: 0.000001, Train Acc: 0.999447, Train F1 Score: 0.999446\n",
      "Train Epoch: 11 [45200/113072 (40%)]\tTrain Loss: 0.000001, Train Acc: 0.999447, Train F1 Score: 0.999447\n",
      "Train Epoch: 11 [67808/113072 (60%)]\tTrain Loss: 0.000000, Train Acc: 0.999281, Train F1 Score: 0.999279\n",
      "Train Epoch: 11 [90416/113072 (80%)]\tTrain Loss: 0.000000, Train Acc: 0.999212, Train F1 Score: 0.999210\n",
      "Train Epoch: 11 [113024/113072 (100%)]\tTrain Loss: 0.000345, Train Acc: 0.999292, Train F1 Score: 0.999292\n",
      "Validation dataset: Val Loss: 0.000492, Val Acc: 0.998302, Val F1 Score: 0.998306\n",
      "Train Epoch: 12 [22592/113072 (20%)]\tTrain Loss: 0.000000, Train Acc: 0.999061, Train F1 Score: 0.999071\n",
      "Train Epoch: 12 [45200/113072 (40%)]\tTrain Loss: 0.000000, Train Acc: 0.999171, Train F1 Score: 0.999177\n",
      "Train Epoch: 12 [67808/113072 (60%)]\tTrain Loss: 0.000211, Train Acc: 0.999245, Train F1 Score: 0.999244\n",
      "Train Epoch: 12 [90416/113072 (80%)]\tTrain Loss: 0.000001, Train Acc: 0.999267, Train F1 Score: 0.999271\n",
      "Train Epoch: 12 [113024/113072 (100%)]\tTrain Loss: 0.000214, Train Acc: 0.999259, Train F1 Score: 0.999263\n",
      "Validation dataset: Val Loss: 0.000411, Val Acc: 0.999045, Val F1 Score: 0.999057\n",
      "Saved Model!!!\n",
      "Train Epoch: 13 [22592/113072 (20%)]\tTrain Loss: 0.000000, Train Acc: 0.999337, Train F1 Score: 0.999342\n",
      "Train Epoch: 13 [45200/113072 (40%)]\tTrain Loss: 0.000000, Train Acc: 0.999115, Train F1 Score: 0.999123\n",
      "Train Epoch: 13 [67808/113072 (60%)]\tTrain Loss: 0.000000, Train Acc: 0.999226, Train F1 Score: 0.999231\n",
      "Train Epoch: 13 [90416/113072 (80%)]\tTrain Loss: 0.000000, Train Acc: 0.999254, Train F1 Score: 0.999256\n",
      "Train Epoch: 13 [113024/113072 (100%)]\tTrain Loss: 0.000629, Train Acc: 0.999270, Train F1 Score: 0.999276\n",
      "Validation dataset: Val Loss: 0.000377, Val Acc: 0.998939, Val F1 Score: 0.998966\n",
      "Train Epoch: 14 [22592/113072 (20%)]\tTrain Loss: 0.000002, Train Acc: 0.999503, Train F1 Score: 0.999490\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[26], line 9\u001b[0m\n\u001b[1;32m      7\u001b[0m train_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, EPOCHS \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m):\n\u001b[0;32m----> 9\u001b[0m     train_step, train_loss, train_acc, train_f1 \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_per_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_step\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     10\u001b[0m     val_loss, val_acc, val_f1 \u001b[38;5;241m=\u001b[39m validation(model, valid_loader, loss_fn, train_step)\n\u001b[1;32m     11\u001b[0m     train_result\u001b[38;5;241m.\u001b[39mappend([train_loss, train_acc, train_f1])\n",
      "Cell \u001b[0;32mIn[9], line 27\u001b[0m, in \u001b[0;36mtrain_per_epoch\u001b[0;34m(epoch, model, train_loader, loss_fn, optimizer, train_step)\u001b[0m\n\u001b[1;32m     24\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m     25\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m---> 27\u001b[0m current_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (batch_idx \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m%\u001b[39m log_interval \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m     30\u001b[0m     train_loss \u001b[38;5;241m=\u001b[39m current_loss \u001b[38;5;241m/\u001b[39m log_interval\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "save_point = 0\n",
    "path = os.path.join(model_path, 'model.ckpt')\n",
    "\n",
    "train_result = []\n",
    "val_result = []\n",
    "\n",
    "train_step = 0\n",
    "for epoch in range(1, EPOCHS + 1):\n",
    "    train_step, train_loss, train_acc, train_f1 = train_per_epoch(epoch, model, train_loader, loss_fn, optimizer, train_step)\n",
    "    val_loss, val_acc, val_f1 = validation(model, valid_loader, loss_fn, train_step)\n",
    "    train_result.append([train_loss, train_acc, train_f1])\n",
    "    val_result.append([val_loss, val_acc, val_f1])\n",
    "\n",
    "    if val_f1 > save_point:\n",
    "        save_point = val_f1\n",
    "        save_dir = os.path.dirname(path)\n",
    "        torch.save(model, os.path.join(save_dir, f'val_f1-{val_f1}-{path.split(\"/\")[-1]}'))\n",
    "        print('Saved Model!!!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model = torch.load('../code/runs/2024-02-19T11:33:53-efficientnet_b4_new_aug_data_16_batch_/models/val_f1-0.9990568947368137-model.ckpt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for valid\n",
    "val_preds_list = []\n",
    "val_targets_list = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    val_loss = 0.0\n",
    "    val_correct = 0.0\n",
    "    best_model.eval()\n",
    "\n",
    "    for val_idx, (val_images, val_labels) in enumerate(valid_loader):\n",
    "        val_images, val_labels = val_images.to(device), val_labels.to(device)\n",
    "\n",
    "        val_outputs = best_model(val_images)\n",
    "        _, val_preds = torch.max(val_outputs, 1)\n",
    "        \n",
    "        val_preds_list.extend(val_preds.detach().cpu().numpy())\n",
    "        val_targets_list.extend(val_labels.detach().cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9997255846571332"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(val_targets_list, val_preds_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_val['pred'] = val_preds_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32788"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_val[df_val['target'] == df_val['pred']].target.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "target\n",
       "3     6\n",
       "14    2\n",
       "11    1\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_val[df_val['target'] != df_val['pred']].target.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>target</th>\n",
       "      <th>pred</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>48348</th>\n",
       "      <td>tf6_r180_5ba19592cee8212c.jpg</td>\n",
       "      <td>14</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60725</th>\n",
       "      <td>tf9_r180_99d7241ecca09eaf.jpg</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>151191</th>\n",
       "      <td>another_tf2_r180_36e1647d484b88b7.jpg</td>\n",
       "      <td>7</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91678</th>\n",
       "      <td>tf9_Hflip_a988e3b816d9aedb.jpg</td>\n",
       "      <td>4</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>153055</th>\n",
       "      <td>another_tf1_r270_7b6175b967d7c92f.jpg</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82883</th>\n",
       "      <td>tf5_Hflip_297bbf30d4be0d50.jpg</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80511</th>\n",
       "      <td>tf6_Hflip_02ebb92c43006832.jpg</td>\n",
       "      <td>11</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>158515</th>\n",
       "      <td>another_tf1_Hflip_r180_1af85d095317707e.jpg</td>\n",
       "      <td>7</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>161178</th>\n",
       "      <td>another_tf4_Hflip_r180_80f998b8dd043ab3.jpg</td>\n",
       "      <td>14</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94167</th>\n",
       "      <td>tf6_Hflip_ce20ee390833e1ed.jpg</td>\n",
       "      <td>4</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>138711</th>\n",
       "      <td>tf6_Hflip_r180_cd0f485de1be0538.jpg</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59464</th>\n",
       "      <td>tf8_r270_91f89a9a3238fc94.jpg</td>\n",
       "      <td>14</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104257</th>\n",
       "      <td>tf2_Vflip_r270_241cfb4ccd3666f0.jpg</td>\n",
       "      <td>7</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>111296</th>\n",
       "      <td>tf1_Hflip_r180_4620f6e53442f3b6.jpg</td>\n",
       "      <td>7</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30200</th>\n",
       "      <td>tf3_r180_02ebb92c43006832.jpg</td>\n",
       "      <td>11</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>157017</th>\n",
       "      <td>another_tf3_Hflip_7c42f2261aedc210.jpg</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>161967</th>\n",
       "      <td>another_tf3_Hflip_r180_a376fbdb67bc4a92.jpg</td>\n",
       "      <td>7</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>161455</th>\n",
       "      <td>another_tf1_Hflip_r180_8646f2c3280a4f49.jpg</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148192</th>\n",
       "      <td>another_tf3_1e5bf6fe5e8686d6.jpg</td>\n",
       "      <td>7</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96956</th>\n",
       "      <td>tf17_Hflip_fb70357d03187bf2.jpg</td>\n",
       "      <td>14</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>139020</th>\n",
       "      <td>tf0_Vflip_r90_cee86faf6225599e.jpg</td>\n",
       "      <td>7</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>118207</th>\n",
       "      <td>tf9_Hflip_r180_654e10bb8ec38099.jpg</td>\n",
       "      <td>4</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30633</th>\n",
       "      <td>tf6_r180_055b74c6c5f5d86b.jpg</td>\n",
       "      <td>14</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>156715</th>\n",
       "      <td>another_tf6_Hflip_5db90ffe1eb7b025.jpg</td>\n",
       "      <td>14</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67615</th>\n",
       "      <td>tf6_r90_bcdf3ca09b310143.jpg</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40333</th>\n",
       "      <td>tf8_r180_37492d1e2cd93ccb.jpg</td>\n",
       "      <td>10</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34916</th>\n",
       "      <td>tf2_r270_1ab6b4d6b1ebe5dd.jpg</td>\n",
       "      <td>7</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33902</th>\n",
       "      <td>tf2_r270_149c8444ae80d630.jpg</td>\n",
       "      <td>13</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84653</th>\n",
       "      <td>tf9_Hflip_418bf4c97eddc30a.jpg</td>\n",
       "      <td>4</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43595</th>\n",
       "      <td>tf2_r270_4620f6e53442f3b6.jpg</td>\n",
       "      <td>7</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105196</th>\n",
       "      <td>tf8_Vflip_r270_2a3790ab1adfe1f4.jpg</td>\n",
       "      <td>3</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>116959</th>\n",
       "      <td>tf8_Vflip_r270_5f047c4d27c6d01e.jpg</td>\n",
       "      <td>7</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131393</th>\n",
       "      <td>tf2_Vflip_r90_a988e3b816d9aedb.jpg</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90833</th>\n",
       "      <td>tf3_Hflip_9bea7d9dc1ce362b.jpg</td>\n",
       "      <td>7</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102287</th>\n",
       "      <td>tf3_Vflip_r270_18dc0fddebcbb811.jpg</td>\n",
       "      <td>13</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25193</th>\n",
       "      <td>tf0_bc4ec38de4b79c75.jpg</td>\n",
       "      <td>13</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 ID  target  pred\n",
       "48348                 tf6_r180_5ba19592cee8212c.jpg      14     3\n",
       "60725                 tf9_r180_99d7241ecca09eaf.jpg       3     4\n",
       "151191        another_tf2_r180_36e1647d484b88b7.jpg       7     3\n",
       "91678                tf9_Hflip_a988e3b816d9aedb.jpg       4    14\n",
       "153055        another_tf1_r270_7b6175b967d7c92f.jpg       3     7\n",
       "82883                tf5_Hflip_297bbf30d4be0d50.jpg       3     7\n",
       "80511                tf6_Hflip_02ebb92c43006832.jpg      11    10\n",
       "158515  another_tf1_Hflip_r180_1af85d095317707e.jpg       7     3\n",
       "161178  another_tf4_Hflip_r180_80f998b8dd043ab3.jpg      14     4\n",
       "94167                tf6_Hflip_ce20ee390833e1ed.jpg       4    14\n",
       "138711          tf6_Hflip_r180_cd0f485de1be0538.jpg       1    10\n",
       "59464                 tf8_r270_91f89a9a3238fc94.jpg      14     4\n",
       "104257          tf2_Vflip_r270_241cfb4ccd3666f0.jpg       7     4\n",
       "111296          tf1_Hflip_r180_4620f6e53442f3b6.jpg       7    14\n",
       "30200                 tf3_r180_02ebb92c43006832.jpg      11    10\n",
       "157017       another_tf3_Hflip_7c42f2261aedc210.jpg       3     7\n",
       "161967  another_tf3_Hflip_r180_a376fbdb67bc4a92.jpg       7    11\n",
       "161455  another_tf1_Hflip_r180_8646f2c3280a4f49.jpg       3     7\n",
       "148192             another_tf3_1e5bf6fe5e8686d6.jpg       7     3\n",
       "96956               tf17_Hflip_fb70357d03187bf2.jpg      14     4\n",
       "139020           tf0_Vflip_r90_cee86faf6225599e.jpg       7     3\n",
       "118207          tf9_Hflip_r180_654e10bb8ec38099.jpg       4     7\n",
       "30633                 tf6_r180_055b74c6c5f5d86b.jpg      14     7\n",
       "156715       another_tf6_Hflip_5db90ffe1eb7b025.jpg      14     7\n",
       "67615                  tf6_r90_bcdf3ca09b310143.jpg       3     7\n",
       "40333                 tf8_r180_37492d1e2cd93ccb.jpg      10     6\n",
       "34916                 tf2_r270_1ab6b4d6b1ebe5dd.jpg       7     3\n",
       "33902                 tf2_r270_149c8444ae80d630.jpg      13     7\n",
       "84653                tf9_Hflip_418bf4c97eddc30a.jpg       4    14\n",
       "43595                 tf2_r270_4620f6e53442f3b6.jpg       7    14\n",
       "105196          tf8_Vflip_r270_2a3790ab1adfe1f4.jpg       3    14\n",
       "116959          tf8_Vflip_r270_5f047c4d27c6d01e.jpg       7    14\n",
       "131393           tf2_Vflip_r90_a988e3b816d9aedb.jpg       4     3\n",
       "90833                tf3_Hflip_9bea7d9dc1ce362b.jpg       7    14\n",
       "102287          tf3_Vflip_r270_18dc0fddebcbb811.jpg      13     3\n",
       "25193                      tf0_bc4ec38de4b79c75.jpg      13     7"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_val[df_val['target'] != df_val['pred']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 197/197 [00:22<00:00,  8.67it/s]\n"
     ]
    }
   ],
   "source": [
    "preds_list = []\n",
    "\n",
    "best_model.eval()\n",
    "for image in tqdm(test_loader):\n",
    "    image = image.to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        preds = best_model(image)\n",
    "    preds_list.extend(preds.argmax(dim=1).detach().cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert (sample_submission_df['ID'] == test_dataset.ids).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_df = pd.DataFrame(test_dataset.ids, columns=['ID'])\n",
    "pred_df['target'] = preds_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_df.to_csv(\"pred17.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
