{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip uninstall -y pillow\n",
        "!CC=\"cc -mavx2\" pip install -U --force-reinstall pillow-simd"
      ],
      "metadata": {
        "id": "zTrkPnFq-GmL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://aistages-api-public-prod.s3.amazonaws.com/app/Competitions/000284/data/data.tar.gz"
      ],
      "metadata": {
        "id": "OMWHTJ8o-H2-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!tar -zxvf data.tar.gz"
      ],
      "metadata": {
        "id": "XbtzEurN-Jpz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NRuxgR8--yGt"
      },
      "outputs": [],
      "source": [
        "from glob import glob\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "from tqdm import tqdm\n",
        "import random\n",
        "import albumentations as A\n",
        "from PIL import Image\n",
        "import cv2\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import backend\n",
        "from tensorflow.keras import layers\n",
        "from torch.utils.data import Dataset, DataLoader"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tf.config.set_soft_device_placement(True)\n",
        "\n",
        "try:\n",
        "  tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n",
        "  print('Device:', tpu.master())\n",
        "  tf.config.experimental_connect_to_cluster(tpu)\n",
        "  tf.tpu.experimental.initialize_tpu_system(tpu)\n",
        "  strategy = tf.distribute.experimental.TPUStrategy(tpu)\n",
        "except:\n",
        "  strategy = tf.distribute.get_strategy()\n",
        "print('Number of replicas', strategy.num_replicas_in_sync)"
      ],
      "metadata": {
        "id": "Sg9KFh9X-LrF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 랜덤 시드 설정\n",
        "seed_value = 1337\n",
        "\n",
        "# Python의 시드 설정\n",
        "random.seed(seed_value)\n",
        "\n",
        "# Numpy의 시드 설정\n",
        "np.random.seed(seed_value)\n",
        "\n",
        "# TensorFlow의 시드 설정\n",
        "tf.random.set_seed(seed_value)"
      ],
      "metadata": {
        "id": "DDI3H8ZWLj39"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "img_width, img_height = 384, 384"
      ],
      "metadata": {
        "id": "HiR5HLv9Bjkm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "transform = A.Compose([\n",
        "    A.ShiftScaleRotate(scale_limit=(0, 0.1), p=0.7),\n",
        "    A.RandomBrightnessContrast(brightness_limit=[-0.3, 0.1], contrast_limit=[-0.3, 0.1], p=1),\n",
        "    A.HorizontalFlip(p=0.5),\n",
        "    A.GaussNoise(var_limit=(10, 50), p=0.5),\n",
        "    A.CoarseDropout(p=0.3, max_holes=15, max_height=15, max_width=15),\n",
        "    A.OneOf([\n",
        "        A.CLAHE(p=0.7),\n",
        "        A.ToGray(p=0.1),\n",
        "        A.Blur(blur_limit=(5, 10), p=0.2)\n",
        "    ], p=1)\n",
        "])"
      ],
      "metadata": {
        "id": "BXY5eCxpBnH8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ROOT_DIR = \"/content/data\"\n",
        "TRAIN_DIR = \"/content/data/train\""
      ],
      "metadata": {
        "id": "xMP_kFuIBods"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_train_file_path(id):\n",
        "    return f\"{TRAIN_DIR}/{id}\"\n",
        "df = pd.read_csv(f\"{ROOT_DIR}/train.csv\")\n",
        "df['file_path'] = df['ID'].apply(get_train_file_path)\n",
        "df.head()"
      ],
      "metadata": {
        "id": "Z2HnEOAD-Nze"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "file_path = df['file_path']\n",
        "target = df['target']"
      ],
      "metadata": {
        "id": "z5qRcrJwB5K9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from PIL import Image\n",
        "import cv2\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tqdm import tqdm"
      ],
      "metadata": {
        "id": "l7lm_41yB6DF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_dataset(dataset_paths, img_width, img_height):\n",
        "    x_data = []\n",
        "    for path in tqdm(dataset_paths):\n",
        "        img = cv2.imread(path)\n",
        "        img = cv2.resize(img, (img_width, img_height))\n",
        "        img = np.asarray(img)\n",
        "        img = tf.keras.applications.efficientnet_v2.preprocess_input(img)\n",
        "        x_data.append(img)\n",
        "\n",
        "    x_data = np.array(x_data)\n",
        "    return x_data\n",
        "\n",
        "x_data = load_dataset(file_path, img_width, img_height)\n",
        "y_data = np.array(target)"
      ],
      "metadata": {
        "id": "Z-QRMAUD-Pct"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "patch_size      = (2,2)   # 2-by-2 sized patches\n",
        "dropout_rate    = 0.5     # Dropout rate\n",
        "num_heads       = 8       # Attention heads\n",
        "embed_dim       = 64      # Embedding dimension\n",
        "num_mlp         = 128     # MLP layer size\n",
        "qkv_bias        = True    # Convert embedded patches to query, key, and values\n",
        "window_size     = 2       # Size of attention window\n",
        "shift_size      = 1       # Size of shifting window\n",
        "image_dimension = 24      # Initial image size / Input size of the transformer model\n",
        "\n",
        "num_patch_x = image_dimension // patch_size[0]\n",
        "num_patch_y = image_dimension // patch_size[1]"
      ],
      "metadata": {
        "id": "C9ZIKGXjB8_Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def window_partition(x, window_size):\n",
        "    _, height, width, channels = x.shape\n",
        "    patch_num_y = height // window_size\n",
        "    patch_num_x = width // window_size\n",
        "    x = tf.reshape(\n",
        "        x, shape=(-1, patch_num_y, window_size, patch_num_x, window_size, channels)\n",
        "    )\n",
        "    x = tf.transpose(x, (0, 1, 3, 2, 4, 5))\n",
        "    windows = tf.reshape(x, shape=(-1, window_size, window_size, channels))\n",
        "    return windows\n",
        "\n",
        "\n",
        "def window_reverse(windows, window_size, height, width, channels):\n",
        "    patch_num_y = height // window_size\n",
        "    patch_num_x = width // window_size\n",
        "    x = tf.reshape(\n",
        "        windows,\n",
        "        shape=(-1, patch_num_y, patch_num_x, window_size, window_size, channels),\n",
        "    )\n",
        "    x = tf.transpose(x, perm=(0, 1, 3, 2, 4, 5))\n",
        "    x = tf.reshape(x, shape=(-1, height, width, channels))\n",
        "    return x\n",
        "\n",
        "\n",
        "class DropPath(layers.Layer):\n",
        "    def __init__(self, drop_prob=None, **kwargs):\n",
        "        super(DropPath, self).__init__(**kwargs)\n",
        "        self.drop_prob = drop_prob\n",
        "\n",
        "    def call(self, inputs, training=None):\n",
        "        if self.drop_prob == 0.0 or not training:\n",
        "            return inputs\n",
        "        else:\n",
        "            batch_size = tf.shape(inputs)[0]\n",
        "            keep_prob = 1 - self.drop_prob\n",
        "            path_mask_shape = (batch_size,) + (1,) * (len(tf.shape(inputs)) - 1)\n",
        "            path_mask = tf.floor(\n",
        "                backend.random_bernoulli(path_mask_shape, p=keep_prob)\n",
        "            )\n",
        "            outputs = (\n",
        "                tf.math.divide(tf.cast(inputs, dtype=tf.float32), keep_prob) * path_mask\n",
        "            )\n",
        "            return outputs\n",
        "\n",
        "    def get_config(self):\n",
        "        config = super().get_config()\n",
        "        config.update(\n",
        "            {\n",
        "                \"drop_prob\": self.drop_prob,\n",
        "            }\n",
        "        )\n",
        "        return config"
      ],
      "metadata": {
        "id": "QAScMfDzCA3G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class PatchExtract(layers.Layer):\n",
        "    def __init__(self, patch_size, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.patch_size_x = patch_size[0]\n",
        "        self.patch_size_y = patch_size[0]\n",
        "\n",
        "    def call(self, images):\n",
        "        batch_size = tf.shape(images)[0]\n",
        "        patches = tf.image.extract_patches(\n",
        "            images=images,\n",
        "            sizes=(1, self.patch_size_x, self.patch_size_y, 1),\n",
        "            strides=(1, self.patch_size_x, self.patch_size_y, 1),\n",
        "            rates=(1, 1, 1, 1),\n",
        "            padding=\"VALID\",\n",
        "        )\n",
        "        patch_dim = patches.shape[-1]\n",
        "        patch_num = patches.shape[1]\n",
        "        return tf.reshape(patches, (batch_size, patch_num * patch_num, patch_dim))\n",
        "\n",
        "    def get_config(self):\n",
        "        config = super().get_config()\n",
        "        config.update(\n",
        "            {\n",
        "                \"patch_size_y\": self.patch_size_y,\n",
        "                \"patch_size_x\": self.patch_size_x,\n",
        "            }\n",
        "        )\n",
        "        return config\n",
        "\n",
        "\n",
        "class PatchEmbedding(layers.Layer):\n",
        "    def __init__(self, num_patch, embed_dim, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.num_patch = num_patch\n",
        "        self.proj = layers.Dense(embed_dim)\n",
        "        self.pos_embed = layers.Embedding(input_dim=num_patch, output_dim=embed_dim)\n",
        "\n",
        "    def call(self, patch):\n",
        "        pos = tf.range(start=0, limit=self.num_patch, delta=1)\n",
        "        return self.proj(patch) + self.pos_embed(pos)\n",
        "\n",
        "    def get_config(self):\n",
        "        config = super().get_config()\n",
        "        config.update(\n",
        "            {\n",
        "                \"num_patch\": self.num_patch,\n",
        "            }\n",
        "        )\n",
        "        return config\n",
        "\n",
        "\n",
        "class PatchMerging(layers.Layer):\n",
        "    def __init__(self, num_patch, embed_dim):\n",
        "        super().__init__()\n",
        "        self.num_patch = num_patch\n",
        "        self.embed_dim = embed_dim\n",
        "        self.linear_trans = layers.Dense(2 * embed_dim, use_bias=False)\n",
        "\n",
        "    def call(self, x):\n",
        "        height, width = self.num_patch\n",
        "        _, _, C = x.get_shape().as_list()\n",
        "        x = tf.reshape(x, shape=(-1, height, width, C))\n",
        "        feat_maps = x\n",
        "\n",
        "        x0 = x[:, 0::2, 0::2, :]\n",
        "        x1 = x[:, 1::2, 0::2, :]\n",
        "        x2 = x[:, 0::2, 1::2, :]\n",
        "        x3 = x[:, 1::2, 1::2, :]\n",
        "        x = tf.concat((x0, x1, x2, x3), axis=-1)\n",
        "        x = tf.reshape(x, shape=(-1, (height // 2) * (width // 2), 4 * C))\n",
        "        return self.linear_trans(x), feat_maps\n",
        "\n",
        "    def get_config(self):\n",
        "        config = super().get_config()\n",
        "        config.update({\"num_patch\": self.num_patch, \"embed_dim\": self.embed_dim})\n",
        "        return config"
      ],
      "metadata": {
        "id": "KoK2ApplCDzB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class WindowAttention(layers.Layer):\n",
        "    def __init__(\n",
        "        self,\n",
        "        dim,\n",
        "        window_size,\n",
        "        num_heads,\n",
        "        qkv_bias=True,\n",
        "        dropout_rate=0.0,\n",
        "        return_attention_scores=False,\n",
        "        **kwargs\n",
        "    ):\n",
        "        super().__init__(**kwargs)\n",
        "        self.dim = dim\n",
        "        self.window_size = window_size\n",
        "        self.num_heads = num_heads\n",
        "        self.scale = (dim // num_heads) ** -0.5\n",
        "        self.return_attention_scores = return_attention_scores\n",
        "        self.qkv = layers.Dense(dim * 3, use_bias=qkv_bias)\n",
        "        self.dropout = layers.Dropout(dropout_rate)\n",
        "        self.proj = layers.Dense(dim)\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        self.relative_position_bias_table = self.add_weight(\n",
        "            shape=(\n",
        "                (2 * self.window_size[0] - 1) * (2 * self.window_size[1] - 1),\n",
        "                self.num_heads,\n",
        "            ),\n",
        "            initializer=\"zeros\",\n",
        "            trainable=True,\n",
        "            name=\"relative_position_bias_table\",\n",
        "        )\n",
        "\n",
        "        self.relative_position_index = self.get_relative_position_index(\n",
        "            self.window_size[0], self.window_size[1]\n",
        "        )\n",
        "        super().build(input_shape)\n",
        "\n",
        "    def get_relative_position_index(self, window_height, window_width):\n",
        "        x_x, y_y = tf.meshgrid(range(window_height), range(window_width))\n",
        "        coords = tf.stack([y_y, x_x], axis=0)\n",
        "        coords_flatten = tf.reshape(coords, [2, -1])\n",
        "\n",
        "        relative_coords = coords_flatten[:, :, None] - coords_flatten[:, None, :]\n",
        "        relative_coords = tf.transpose(relative_coords, perm=[1, 2, 0])\n",
        "\n",
        "        x_x = (relative_coords[:, :, 0] + window_height - 1) * (2 * window_width - 1)\n",
        "        y_y = relative_coords[:, :, 1] + window_width - 1\n",
        "        relative_coords = tf.stack([x_x, y_y], axis=-1)\n",
        "\n",
        "        return tf.reduce_sum(relative_coords, axis=-1)\n",
        "\n",
        "    def call(self, x, mask=None):\n",
        "        _, size, channels = x.shape\n",
        "        head_dim = channels // self.num_heads\n",
        "        x_qkv = self.qkv(x)\n",
        "        x_qkv = tf.reshape(x_qkv, shape=(-1, size, 3, self.num_heads, head_dim))\n",
        "        x_qkv = tf.transpose(x_qkv, perm=(2, 0, 3, 1, 4))\n",
        "        q, k, v = x_qkv[0], x_qkv[1], x_qkv[2]\n",
        "        q = q * self.scale\n",
        "        k = tf.transpose(k, perm=(0, 1, 3, 2))\n",
        "        attn = q @ k\n",
        "\n",
        "        relative_position_bias = tf.gather(\n",
        "            self.relative_position_bias_table,\n",
        "            self.relative_position_index,\n",
        "            axis=0,\n",
        "        )\n",
        "        relative_position_bias = tf.transpose(relative_position_bias, [2, 0, 1])\n",
        "        attn = attn + tf.expand_dims(relative_position_bias, axis=0)\n",
        "\n",
        "        if mask is not None:\n",
        "            nW = mask.get_shape()[0]\n",
        "            mask_float = tf.cast(\n",
        "                tf.expand_dims(tf.expand_dims(mask, axis=1), axis=0), tf.float32\n",
        "            )\n",
        "            attn = (\n",
        "                tf.reshape(attn, shape=(-1, nW, self.num_heads, size, size))\n",
        "                + mask_float\n",
        "            )\n",
        "            attn = tf.reshape(attn, shape=(-1, self.num_heads, size, size))\n",
        "            attn = tf.nn.softmax(attn, axis=-1)\n",
        "        else:\n",
        "            attn = tf.nn.softmax(attn, axis=-1)\n",
        "        attn = self.dropout(attn)\n",
        "\n",
        "        x_qkv = attn @ v\n",
        "        x_qkv = tf.transpose(x_qkv, perm=(0, 2, 1, 3))\n",
        "        x_qkv = tf.reshape(x_qkv, shape=(-1, size, channels))\n",
        "        x_qkv = self.proj(x_qkv)\n",
        "        x_qkv = self.dropout(x_qkv)\n",
        "\n",
        "        if self.return_attention_scores:\n",
        "            return x_qkv, attn\n",
        "        else:\n",
        "            return x_qkv\n",
        "\n",
        "    def get_config(self):\n",
        "        config = super().get_config()\n",
        "        config.update(\n",
        "            {\n",
        "                \"dim\": self.dim,\n",
        "                \"window_size\": self.window_size,\n",
        "                \"num_heads\": self.num_heads,\n",
        "                \"scale\": self.scale,\n",
        "            }\n",
        "        )\n",
        "        return config"
      ],
      "metadata": {
        "id": "zFf3xwdzCFSl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import jax\n",
        "from jax import jit\n",
        "from jax import random\n",
        "from jax import numpy as jnp\n",
        "from jax.experimental import jax2tf\n",
        "\n",
        "class SwinTransformer(layers.Layer):\n",
        "    def __init__(\n",
        "        self,\n",
        "        dim,\n",
        "        num_patch,\n",
        "        num_heads,\n",
        "        window_size=7,\n",
        "        shift_size=0,\n",
        "        num_mlp=1024,\n",
        "        qkv_bias=True,\n",
        "        dropout_rate=0.0,\n",
        "        **kwargs,\n",
        "    ):\n",
        "        super(SwinTransformer, self).__init__(**kwargs)\n",
        "\n",
        "        self.dim = dim\n",
        "        self.num_patch = num_patch\n",
        "        self.num_heads = num_heads\n",
        "        self.window_size = window_size\n",
        "        self.shift_size = shift_size\n",
        "        self.num_mlp = num_mlp\n",
        "\n",
        "        self.norm1 = layers.LayerNormalization(epsilon=1e-5)\n",
        "        self.attn = WindowAttention(\n",
        "            dim,\n",
        "            window_size=(self.window_size, self.window_size),\n",
        "            num_heads=num_heads,\n",
        "            qkv_bias=qkv_bias,\n",
        "            dropout_rate=dropout_rate,\n",
        "        )\n",
        "        self.drop_path = (\n",
        "            DropPath(dropout_rate) if dropout_rate > 0.0 else tf.identity\n",
        "        )\n",
        "        self.norm2 = layers.LayerNormalization(epsilon=1e-5)\n",
        "\n",
        "        self.mlp = keras.Sequential(\n",
        "            [\n",
        "                layers.Dense(num_mlp),\n",
        "                layers.Activation(keras.activations.gelu),\n",
        "                layers.Dropout(dropout_rate),\n",
        "                layers.Dense(dim),\n",
        "                layers.Dropout(dropout_rate),\n",
        "            ]\n",
        "        )\n",
        "\n",
        "        if min(self.num_patch) < self.window_size:\n",
        "            self.shift_size = 0\n",
        "            self.window_size = min(self.num_patch)\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        if self.shift_size == 0:\n",
        "            self.attn_mask = None\n",
        "        else:\n",
        "            height, width = self.num_patch\n",
        "            h_slices = (\n",
        "                slice(0, -self.window_size),\n",
        "                slice(-self.window_size, -self.shift_size),\n",
        "                slice(-self.shift_size, None),\n",
        "            )\n",
        "            w_slices = (\n",
        "                slice(0, -self.window_size),\n",
        "                slice(-self.window_size, -self.shift_size),\n",
        "                slice(-self.shift_size, None),\n",
        "            )\n",
        "            mask_array = jnp.zeros((1, height, width, 1))\n",
        "            count = 0\n",
        "            for h in h_slices:\n",
        "                for w in w_slices:\n",
        "                    mask_array[:, h, w, :] = count\n",
        "                    count += 1\n",
        "            mask_array = tf.convert_to_tensor(mask_array)\n",
        "\n",
        "            # mask array to windows\n",
        "            mask_windows = window_partition(mask_array, self.window_size)\n",
        "            mask_windows = tf.reshape(\n",
        "                mask_windows, shape=[-1, self.window_size * self.window_size]\n",
        "            )\n",
        "            attn_mask = tf.expand_dims(mask_windows, axis=1) - tf.expand_dims(\n",
        "                mask_windows, axis=2\n",
        "            )\n",
        "            attn_mask = tf.where(attn_mask != 0, -100.0, attn_mask)\n",
        "            attn_mask = tf.where(attn_mask == 0, 0.0, attn_mask)\n",
        "            self.attn_mask = tf.Variable(initial_value=attn_mask, trainable=False)\n",
        "\n",
        "    def call(self, x):\n",
        "        height, width = self.num_patch\n",
        "        _, num_patches_before, channels = x.shape\n",
        "        x_skip = x\n",
        "        x = self.norm1(x)\n",
        "        x = tf.reshape(x, shape=(-1, height, width, channels))\n",
        "        if self.shift_size > 0:\n",
        "            shifted_x = tf.roll(\n",
        "                x, shift=[-self.shift_size, -self.shift_size], axis=[1, 2]\n",
        "            )\n",
        "        else:\n",
        "            shifted_x = x\n",
        "\n",
        "        x_windows = window_partition(shifted_x, self.window_size)\n",
        "        x_windows = tf.reshape(\n",
        "            x_windows, shape=(-1, self.window_size * self.window_size, channels)\n",
        "        )\n",
        "        attn_windows = self.attn(x_windows, mask=self.attn_mask)\n",
        "\n",
        "        attn_windows = tf.reshape(\n",
        "            attn_windows, shape=(-1, self.window_size, self.window_size, channels)\n",
        "        )\n",
        "        shifted_x = window_reverse(\n",
        "            attn_windows, self.window_size, height, width, channels\n",
        "        )\n",
        "        if self.shift_size > 0:\n",
        "            x = tf.roll(\n",
        "                shifted_x, shift=[self.shift_size, self.shift_size], axis=[1, 2]\n",
        "            )\n",
        "        else:\n",
        "            x = shifted_x\n",
        "\n",
        "        x = tf.reshape(x, shape=(-1, height * width, channels))\n",
        "        x = self.drop_path(x)\n",
        "        x = tf.cast(x_skip, dtype=tf.float32) + tf.cast(x, dtype=tf.float32)\n",
        "        x_skip = x\n",
        "        x = self.norm2(x)\n",
        "        x = self.mlp(x)\n",
        "        x = self.drop_path(x)\n",
        "        x = tf.cast(x_skip, dtype=tf.float32) + tf.cast(x, dtype=tf.float32)\n",
        "        return x"
      ],
      "metadata": {
        "id": "HVspbqOLCGv9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_step_sam(self, data, rho=0.05):\n",
        "    \"\"\"\n",
        "    Overrides the train_step method of Model\n",
        "\n",
        "    Args:\n",
        "        data : Data on which model is to be trained\n",
        "        rho  : Hyperparameter Rho indicating the size of neighborhood\n",
        "    \"\"\"\n",
        "\n",
        "    sample_weight = None\n",
        "    x, y = data\n",
        "\n",
        "    # Opening Gradient Tape scope to record operations during 1st forward pass\n",
        "    with tf.GradientTape() as tape:\n",
        "        y_pred = self(x, training=True)\n",
        "        # Calculating loss to calculate gradients\n",
        "        loss = self.compiled_loss(y, y_pred, sample_weight=sample_weight, regularization_losses=self.losses)\n",
        "\n",
        "\n",
        "    trainable_vars = self.trainable_variables\n",
        "    # Calculating gradients with respect trainable variable\n",
        "    gradients = tape.gradient(loss, trainable_vars)\n",
        "\n",
        "    \"\"\"\n",
        "    This is the first step which involves calculating the point w_adv with highest loss and virtually moving to that point so that we can get gradient at that point.\n",
        "    \"\"\"\n",
        "    eps_w_ls = [] # list to store the updates done to trainable variables in first step\n",
        "\n",
        "    #computing the norm of gradients which is required for computing eps_w\n",
        "    grad_norm = tf.linalg.global_norm(gradients)\n",
        "\n",
        "    # Iterating over trainable_vars\n",
        "    for i in range(len(trainable_vars)):\n",
        "        # we will calculate eps_w to find w_adv point having highest loss in rho neighborhood\n",
        "        eps_w = tf.math.multiply(gradients[i], rho / grad_norm )\n",
        "        # temporarily moving to w_adv point\n",
        "        trainable_vars[i].assign_add(eps_w)\n",
        "        # storing updates done in eps_w_ls list\n",
        "        eps_w_ls.append(eps_w)\n",
        "\n",
        "    # Opening Gradient Tape scope to record operations during 2nd forward pass\n",
        "    with tf.GradientTape() as tape:\n",
        "        y_pred = self(x, training=True)\n",
        "        # Calculating loss to calculate gradient at w_adv point\n",
        "        loss = self.compiled_loss(y, y_pred, sample_weight=sample_weight, regularization_losses=self.losses)\n",
        "\n",
        "    trainable_vars = self.trainable_variables\n",
        "    #computing gradient at w_adv which is our objective in this first step\n",
        "    gradients = tape.gradient(loss, trainable_vars)\n",
        "\n",
        "    \"\"\"\n",
        "    This is the second step in SAM where we will do actual update at the initial point from the gradient calculated at adversial point w_adv\n",
        "    \"\"\"\n",
        "\n",
        "    for i in range(len(trainable_vars)):\n",
        "        # Going back to orignal parameters\n",
        "        trainable_vars[i].assign_sub(eps_w_ls[i])\n",
        "\n",
        "    # Updating parameters with gradients computed at w_adv\n",
        "    self.optimizer.apply_gradients(zip(gradients, trainable_vars))\n",
        "\n",
        "    # Updating the metrics.\n",
        "    self.compiled_metrics.update_state(y, y_pred, sample_weight=sample_weight)\n",
        "\n",
        "    # returns a dictionary mapping metric names (including the loss) to their current value.\n",
        "    return {m.name: m.result() for m in self.metrics}"
      ],
      "metadata": {
        "id": "AygOPEcWCLZv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D, Dropout, BatchNormalization, LayerNormalization\n",
        "from tensorflow.keras import regularizers\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.applications.efficientnet_v2 import EfficientNetV2M\n",
        "from sklearn.metrics import f1_score\n",
        "from tensorflow.keras.losses import categorical_crossentropy\n",
        "\n",
        "with strategy.scope():\n",
        "    base = EfficientNetV2M(weights='imagenet', include_top=False, input_shape=(img_height, img_width, 3))\n",
        "\n",
        "    class HybridModel(keras.Model):\n",
        "        def __init__(self, model_name, **kwargs):\n",
        "            super().__init__(name=model_name, **kwargs)\n",
        "\n",
        "            # base model with compatible output which will be an input of transformer model\n",
        "            self.multi_output_cnn = keras.Model(\n",
        "                [base.inputs],\n",
        "                [base.get_layer(\"block6a_expand_activation\").output, base.output],\n",
        "                name=\"efficientnet\",\n",
        "            )\n",
        "\n",
        "            # base model's (cnn model) head\n",
        "            self.conv_head = keras.Sequential(\n",
        "                [\n",
        "                    layers.GlobalAveragePooling2D(),\n",
        "                    layers.AlphaDropout(0.5),\n",
        "                    layers.LayerNormalization()\n",
        "                ],\n",
        "                name=\"conv_head\",\n",
        "            )\n",
        "\n",
        "            # stuff of swin transformers\n",
        "            self.patch_extract = PatchExtract(patch_size)\n",
        "            self.patch_embedds = PatchEmbedding(num_patch_x * num_patch_y, embed_dim)\n",
        "            self.patch_merging = PatchMerging(\n",
        "                (num_patch_x, num_patch_y), embed_dim=embed_dim\n",
        "            )\n",
        "\n",
        "            # swin blocks containers\n",
        "            self.swin_sequences = keras.Sequential(name=\"swin_blocks\")\n",
        "            for i in range(shift_size):\n",
        "                self.swin_sequences.add(\n",
        "                    SwinTransformer(\n",
        "                        dim=embed_dim,\n",
        "                        num_patch=(num_patch_x, num_patch_y),\n",
        "                        num_heads=num_heads,\n",
        "                        window_size=window_size,\n",
        "                        shift_size=i,\n",
        "                        num_mlp=num_mlp,\n",
        "                        qkv_bias=qkv_bias,\n",
        "                        dropout_rate=dropout_rate,\n",
        "                    )\n",
        "                )\n",
        "\n",
        "            # swin block's head\n",
        "            self.swin_head = keras.Sequential(\n",
        "                [\n",
        "                    layers.GlobalAveragePooling1D(),\n",
        "                    layers.AlphaDropout(0.5),\n",
        "                    layers.LayerNormalization(),\n",
        "                ],\n",
        "                name=\"swin_head\",\n",
        "            )\n",
        "\n",
        "            # classifier\n",
        "            self.classifier = layers.Dense(\n",
        "                17, activation='softmax'\n",
        "            )\n",
        "\n",
        "            # build the graph\n",
        "            self.build_graph()\n",
        "\n",
        "        def forward_cnn(self, inputs):\n",
        "            # CNN model.\n",
        "            return self.multi_output_cnn(inputs)\n",
        "\n",
        "        def forward_transformer(self, inputs):\n",
        "            # Transformer model.\n",
        "            x = self.patch_extract(inputs)\n",
        "            x = self.patch_embedds(x)\n",
        "            x = self.swin_sequences(tf.cast(x, dtype=tf.float32))\n",
        "            x, swin_gcam_top = self.patch_merging(x)\n",
        "            return x, swin_gcam_top\n",
        "\n",
        "        def call(self, inputs, training=None, **kwargs):\n",
        "            cnn_mid_layer, cnn_gcam_top = self.forward_cnn(inputs)\n",
        "            transformer_output, transformer_gcam_top = self.forward_transformer(\n",
        "                cnn_mid_layer\n",
        "            )\n",
        "\n",
        "            transformer_output = self.swin_head(transformer_output)\n",
        "            cnn_output = self.conv_head(cnn_gcam_top)\n",
        "            logits = self.classifier(tf.concat([transformer_output, cnn_output], axis=-1))\n",
        "\n",
        "            return logits\n",
        "\n",
        "        def build_graph(self):\n",
        "            x = keras.Input(shape=(img_width, img_height, 3))\n",
        "            return keras.Model(inputs=[x], outputs=self.call(x))\n",
        "\n",
        "        # overriding the train_step method  with our custom train_step_sam created in earlier cell\n",
        "        def train_step(self, data):\n",
        "            return train_step_sam(self, data, rho=0.05) # using rho as 0.05 you can tune this hyperparameter\n",
        "\n",
        "    # focal loss 정의\n",
        "    def focal_loss(gamma=2.0, alpha=0.25):\n",
        "        def loss(y_true, y_pred):\n",
        "            ce_loss = categorical_crossentropy(y_true, y_pred)\n",
        "            pt = tf.math.exp(-ce_loss)\n",
        "            focal_loss = alpha * tf.math.pow(1. - pt, gamma) * ce_loss\n",
        "            return focal_loss\n",
        "        return loss\n",
        "\n",
        "    model = HybridModel(\"efficientnet\")\n",
        "    model.build(input_shape=(None, img_height, img_width, 3))\n",
        "\n",
        "    model.compile(\n",
        "        optimizer = tf.keras.optimizers.AdamW(learning_rate=0.0001),\n",
        "        loss = focal_loss(),\n",
        "        metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "kAI4MI9WCOYq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.summary()"
      ],
      "metadata": {
        "id": "5HIoYdrL-TX9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import StratifiedKFold\n",
        "\n",
        "# 콜백들\n",
        "earlystopping = tf.keras.callbacks.EarlyStopping(\n",
        "    monitor='val_loss',\n",
        "    mode='min',\n",
        "    min_delta = 0.001,\n",
        "    patience=8,\n",
        "    restore_best_weights=True,\n",
        "    verbose=0)\n",
        "\n",
        "reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=5)\n",
        "\n",
        "kfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "checkpoint_dir = f'/content/Checkpoints'\n",
        "best_f1_score = 0\n",
        "best_weights = None\n",
        "\n",
        "for fold, (train_index, val_index) in enumerate(kfold.split(x_data, y_data)):\n",
        "\n",
        "    # 학습 데이터와 검증 데이터를 나누기\n",
        "    x_train, y_train = x_data[train_index], y_data[train_index]\n",
        "    x_val, y_val = x_data[val_index], y_data[val_index]\n",
        "\n",
        "    model.fit(x_train, tf.keras.utils.to_categorical(y_train),\n",
        "                batch_size=32,\n",
        "                epochs=30,\n",
        "                validation_data=(x_val, tf.keras.utils.to_categorical(y_val)),\n",
        "                callbacks=[earlystopping, reduce_lr])\n",
        "\n",
        "    # 모델 예측\n",
        "    y_pred = model.predict(x_val)\n",
        "    y_pred_label = np.argmax(y_pred, axis=1)\n",
        "\n",
        "    # 모델 평가\n",
        "    score_f1 = f1_score(y_val, y_pred_label, average='macro')\n",
        "    print(f'Fold {fold+1} - f1_score:', score_f1)\n",
        "\n",
        "    # 가장 좋은 f1_score를 보인 모델 저장\n",
        "    if score_f1 > best_f1_score:\n",
        "        best_f1_score = score_f1\n",
        "        best_weights = model.get_weights()\n",
        "\n",
        "# 모델 checkpoint 저장\n",
        "checkpoint_name = f'{checkpoint_dir}/'\n",
        "model.set_weights(best_weights)\n",
        "model.save_weights(checkpoint_name)"
      ],
      "metadata": {
        "id": "SK0f868--U89"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}